{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/ \n",
    "\n",
    "![TensorFlow](images/tensorflow.png)\n",
    "\n",
    "# Learning TensorFlow\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install \n",
    "\n",
    "Install using conda in linux\n",
    "```bash\n",
    "conda create -n tensorflow python=3.5\n",
    "source activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activate/Deactivate python env\n",
    "\n",
    "\n",
    "```bash\n",
    "source activate tensorflow\n",
    "deactivate tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the first program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "# Download the mnist dataset. \n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World !'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "helloworld = tf.constant(\"Hello World !\") # Objects created by TF is call tensor\n",
    "output = tf.Session().run(helloworld)     # Run the constant in the TF session\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tensor\n",
    "\n",
    "A mathematical object analogous to but more general than a vector, represented by an array of components that are functions of the coordinates of a space. **The central unit of data in TensorFlow is the tensor**. Everything store in TF as a tensor object.\n",
    "\n",
    "- **Rank 0 Tensor**\n",
    "\n",
    "Scaler is a rank 0 tensor and in TF it represent as follows \n",
    "\n",
    "```\n",
    "A = tf.constant(123)\n",
    "``` \n",
    "\n",
    "*This is a constant tensor that does not change*\n",
    "\n",
    "- **Rank 1 Tensor**\n",
    "\n",
    "Vector is a rank 1 tensor and in TF it represents as follows \n",
    "\n",
    "```\n",
    "B = ft.constant([23, 49, 42])\n",
    "```\n",
    "\n",
    "- **Rank 2 Tensor**\n",
    "\n",
    "Matrix is a rank 2 tensor and in TF it represent as follows\n",
    "\n",
    "```\n",
    "C = ft.constant([[23,21,89], [34,982,83]])\n",
    "```\n",
    "\n",
    "- **Rank 3 Tensor**\n",
    "\n",
    "3D array can be represented as a rank 3 tensor and in TF it represent as follows\n",
    "\n",
    "```\n",
    "D = ft.constant([[[1., 2., 3.]], [[7., 8., 9.]]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Session\n",
    "\n",
    "TF api is build around the idea of building a computational grpah of notes and running them. A session encapsulates the control and state of the TensorFlow runtime. \n",
    "\n",
    "\n",
    "#### Work with constant tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0) \n",
    "print(node1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add *node1* and *node2*. This will procuce a another tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node3 = tf.add(node1, node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tf.add.run.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(node3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work with non-constant tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)  # x is a Tensor variable that takes integer value\n",
    "y = tf.placeholder(tf.float32)  # y is a Tensor variable that takes integer value\n",
    "sum_xy = x + y # this is same as tf.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(sum_xy, feed_dict={x:10, y:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(sum_xy, feed_dict={x:2, y:2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Variables \n",
    "\n",
    "TF variables can be modify as usual variables.\n",
    "Regular variables does not get initialize when creating them, you have to call the *global_variables_initializer()* method and run that within the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(5) # x is initialize with 5 and can be modify\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.add(5, 2)  # 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = tf.subtract(10, 4) # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = tf.multiply(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 10]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run([a, b, c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Casting in TF\n",
    "\n",
    "Type cast float to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(2.0) # Floating point number \n",
    "print(sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "y = tf.cast(x, tf.int32) # Integer number\n",
    "print(sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Define Linear Function \n",
    "\n",
    "$y=xW + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](images/linearfunction.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the input variable $x$\n",
    "\n",
    "\n",
    "$x_i$ is an input. For example this can be a vector of an image. So lets assume this is a vector length of 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](images/x_matrix.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_data_vectors = 120 # this could be the number of images I have in my data set\n",
    "length_of_data_vector = 6 # length of the ibserved data vector\n",
    "number_of_categories = 5 # this is the number of categories I have to categorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Randomly generating $x_i$ vectors just for the demonstration. In real project these are the observed data (ex, image with (20px) x (20px) = (300px))_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45693526, -0.52290267,  1.3766675 , -0.27219331,  1.14847934,\n",
       "         0.87284625],\n",
       "       [-1.28326416, -0.63142151,  1.43161869,  1.36415756, -0.79241031,\n",
       "         1.11476231],\n",
       "       [ 0.6918447 , -0.76002383, -0.12883557, -0.61266303,  0.21287961,\n",
       "         1.67242241],\n",
       "       [-0.14879832, -1.96100414,  0.17042372,  0.97559816,  0.72694212,\n",
       "         0.36855826],\n",
       "       [ 0.10577615,  1.41718757,  0.01636093, -0.94101745, -1.8351202 ,\n",
       "         0.23742022],\n",
       "       [ 0.56623954,  0.37197945, -0.10749652, -0.11388438,  1.10594249,\n",
       "        -0.05199287],\n",
       "       [-1.00691795, -0.4253588 ,  0.30210772,  0.33538565,  0.04371909,\n",
       "         1.69481599],\n",
       "       [ 0.08324673, -0.52695876,  0.19547546,  0.58698237,  0.57427913,\n",
       "        -0.66436243],\n",
       "       [-0.62323833, -1.92079937,  0.91602671, -0.6252895 ,  0.78574425,\n",
       "         1.99637783],\n",
       "       [-1.6445117 , -1.76367831, -1.78615165, -0.20332737,  1.77431822,\n",
       "         0.52247059]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(tf.truncated_normal((number_of_data_vectors, length_of_data_vector)))\n",
    "sess.run(tf.global_variables_initializer()) # Initialize the variable x in the session (sess)\n",
    "sess.run(x)[:10] # Print the first 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define weights $W$\n",
    "\n",
    "Initializing the weights with random values is important. Randomizing weights help the model get stuck in local minima everytime we train it. Choosing random weights from a normal distribution is a good practice. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/w_matrix.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.15524042, -0.07092341,  1.57022703, -1.08708596, -0.00983607],\n",
       "       [-1.58749902, -0.26416558, -0.8540172 ,  0.38636714,  0.32315025],\n",
       "       [ 0.37003389, -0.66748691,  0.02387263, -0.80765718, -1.68267286],\n",
       "       [ 1.24784064,  0.44291234, -0.42946509, -0.41863248,  0.36794561],\n",
       "       [-0.83994818,  1.56017733, -0.48999667, -0.5300253 , -1.06569219],\n",
       "       [ 1.08756256,  0.84270841, -1.73785377,  0.88978809, -1.71383536]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = tf.Variable(tf.truncated_normal((length_of_data_vector, number_of_categories)))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define biases $b$\n",
    "\n",
    "Biases also can be initialize with random values. Since weights are initialize with random values, we can define biases with zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/b.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = tf.Variable(tf.zeros(number_of_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer()) \n",
    "sess.run(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the linear function   \n",
    "$y=xW + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/linear.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.21003699,  1.57533538,  0.76778084, -0.08080262,  0.38994288],\n",
       "       [ 0.10521334, -2.03897095, -0.10381129,  1.59654737, -2.12001419],\n",
       "       [ 2.67974472,  0.5302121 ,  0.70923263,  0.91936636, -1.91123331],\n",
       "       [ 0.06339495,  1.68909192,  1.76008499, -0.25631928,  3.09792948],\n",
       "       [-1.24846613, -3.65432572, -1.36266029,  1.56352353, -0.31966019],\n",
       "       [ 2.00285029, -1.44086969,  1.31967235,  1.45819366, -2.31067204],\n",
       "       [ 1.04547918, -0.48274046,  1.90127373,  3.09686327, -1.12880874],\n",
       "       [-0.77865219, -1.7714349 ,  0.35757378,  0.81294727, -1.32572901],\n",
       "       [-2.71481752, -2.95453715,  0.03781496,  1.68836153, -2.41609144],\n",
       "       [ 1.00461364, -0.07493253,  0.37313727, -2.09890938, -1.20498121]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.add(tf.matmul(x, W), b)\n",
    "sess.run(y)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the softmax\n",
    "\n",
    "Softmax function scale $y$ to be between $0$ and $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/softmax.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/softmaxy.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00428082,  0.51261044,  0.22859724,  0.09784438,  0.15666719],\n",
       "       [ 0.15433052,  0.01808191,  0.12521996,  0.68569332,  0.01667431],\n",
       "       [ 0.69538766,  0.08103952,  0.09692692,  0.11959262,  0.00705327],\n",
       "       [ 0.03025218,  0.15374035,  0.16505161,  0.02197387,  0.62898201],\n",
       "       [ 0.04726622,  0.00426284,  0.04216548,  0.78665167,  0.11965372],\n",
       "       [ 0.46939933,  0.01499526,  0.23705114,  0.27227083,  0.00628353],\n",
       "       [ 0.08724091,  0.01892443,  0.20529906,  0.6786173 ,  0.00991834],\n",
       "       [ 0.10024288,  0.03714442,  0.31225556,  0.49235275,  0.05800442],\n",
       "       [ 0.00994737,  0.00782708,  0.15601324,  0.8128019 ,  0.01341045],\n",
       "       [ 0.49353677,  0.16767895,  0.26246583,  0.02215525,  0.05416325]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.nn.softmax(y)    \n",
    "sess.run(y)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clculate Cross Entropy in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "<tf.Variable 'Variable_13:0' shape=(120, 5) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# start from here \n",
    "y_hat_data = mnist.test.labels.astype(np.float32) # just to simulate, I am loading lables from mist dataset\n",
    "y_hat_data = y_hat_data[:,[1, 2, 3, 4, 5]][:120]\n",
    "print(y_hat_data[:10]) # show the first 10 rows \n",
    "y_hat = tf.Variable(y_hat_data)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/crossentropydata.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crosse = -tf.reduce_sum(tf.multiply(tf.log(y), y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batching \n",
    "\n",
    "It takes so much memory to train a model with large dataset. This may be not be realaistic even with less expensive memory. As a solution, we can use the mini batching: take subsets of data from the whole dataset and train the model for each subset. This is not ideal but its allows us to train the model with less amount of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "# mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data in to batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "\n",
    "![](images/graph1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    " \n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model using entier dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.10689999908208847\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Use the enter dataset\n",
    "    sess.run(optimizer, feed_dict={features: train_features, labels: train_labels})\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model using Mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batches\n",
    "![](images/batch.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.06440000236034393\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)    \n",
    "    # Train the optimizer for each batch\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Epoch is a single training (forward and backward pass) of the entier dataset. If we are using mini-batches, we have to train with all the batches within a epoch. \n",
    "- This increase the accuracy of th model without requiring more data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/epochs.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
